// Brad Pitney
// Yin Shi
// ECE 579
// Winter 2014

// KinectManager handles connection to the Microsoft Kinect sensor hardware, and 
// all of the events generated by this device. This includes speech recognition, 
// gesture recognition, and forwarding of skeleton joint data to the person 
// tracking code in ServoManager.

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.IO;
using Microsoft.Kinect;
using Microsoft.Speech.AudioFormat;
using Microsoft.Speech.Recognition;
using System.Speech.Synthesis;

namespace CountessQuantaControl
{
    public class KinectManager
    {
        KinectSensor sensor;
        SpeechRecognitionEngine speechEngine;
        SequenceProcessor sequenceProcessor;
        PersonTracking personTracking;
        RobotSpeech robotSpeech;

        Object speechLock = new Object();
        bool synthesizerIsSpeaking = false;

        bool speechRecognitionEnabled = false;
        bool gestureRecognitionEnabled = false;

        public KinectManager(SequenceProcessor sequenceProcessor, PersonTracking personTracking, RobotSpeech robotSpeech)
        {
            this.sequenceProcessor = sequenceProcessor;
            this.personTracking = personTracking;
            this.robotSpeech = robotSpeech;
        }

        // Connect to the Kinect sensor and begin processing events.
        public void InitializeKinect()
        {
            this.sensor = null;

            // Look through all sensors and start the first connected one.
            // This requires that a Kinect is connected at the time of app startup.
            // To make your app robust against plug/unplug, 
            // it is recommended to use KinectSensorChooser provided in Microsoft.Kinect.Toolkit (See components in Toolkit Browser).
            foreach (var potentialSensor in KinectSensor.KinectSensors)
            {
                if (potentialSensor.Status == KinectStatus.Connected)
                {
                    this.sensor = potentialSensor;
                    break;  // Connect to first Kinect.
                }
            }

            if (null == this.sensor)
            {
                ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Error, "InitializeKinect() failed, not connected to Kinect sensor.");
                return;
            }
            else
            {
                // Turn on the skeleton stream to receive skeleton frames
                this.sensor.SkeletonStream.Enable();

                // Add an event handler to be called whenever there is new color frame data
                SkeletonFrameReady += SensorSkeletonFrameReady;

                // Start the sensor!
                try
                {
                    this.sensor.Start();
                }
                catch (IOException)
                {
                    ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Error, "InitializeKinect() failed, unable to Start Kinect sensor.");
                    this.sensor = null;
                    return;
                }
            }

            RecognizerInfo ri = GetKinectRecognizer();

            if (null != ri)
            {
                this.speechEngine = new SpeechRecognitionEngine(ri.Id);

                /****************************************************************
                * 
                * Use this code to create grammar programmatically rather than from
                * a grammar file.
                * 
                * var directions = new Choices();
                * directions.Add(new SemanticResultValue("forward", "FORWARD"));
                * directions.Add(new SemanticResultValue("forwards", "FORWARD"));
                * directions.Add(new SemanticResultValue("straight", "FORWARD"));
                * directions.Add(new SemanticResultValue("backward", "BACKWARD"));
                * directions.Add(new SemanticResultValue("backwards", "BACKWARD"));
                * directions.Add(new SemanticResultValue("back", "BACKWARD"));
                * directions.Add(new SemanticResultValue("turn left", "LEFT"));
                * directions.Add(new SemanticResultValue("turn right", "RIGHT"));
                *
                * var gb = new GrammarBuilder { Culture = ri.Culture };
                * gb.Append(directions);
                *
                * var g = new Grammar(gb);
                * 
                ****************************************************************/

                // Create a grammar from grammar definition XML file.
                using (var memoryStream = new MemoryStream(Encoding.ASCII.GetBytes(Properties.Resources.SpeechGrammar)))
                {
                    var g = new Grammar(memoryStream);
                    speechEngine.LoadGrammar(g);
                }

                speechEngine.SpeechRecognized += SpeechRecognized;
                speechEngine.SpeechRecognitionRejected += SpeechRejected;

                robotSpeech.SpeakStarted += SpeakStarted;
                robotSpeech.SpeakCompleted += SpeakCompleted;

                // For long recognition sessions (a few hours or more), it may be beneficial to turn off adaptation of the acoustic model. 
                // This will prevent recognition accuracy from degrading over time.
                ////speechEngine.UpdateRecognizerSetting("AdaptationOn", 0);

                speechEngine.SetInputToAudioStream(
                    sensor.AudioSource.Start(), new SpeechAudioFormatInfo(EncodingFormat.Pcm, 16000, 16, 1, 32000, 2, null));
                speechEngine.RecognizeAsync(RecognizeMode.Multiple);
            }
            else
            {
                ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Error, "InitializeKinect() failed, no speech recognizer.");
            }

            ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Info, "InitializeKinect() succeeded, Kinect sensor is ready.");
        }

        public bool IsConnected()
        {
            return (this.sensor != null);
        }

        public void EnableSpeechRecognition(bool enable)
        {
            speechRecognitionEnabled = enable;
        }

        public void EnableGestureRecognition(bool enable)
        {
            gestureRecognitionEnabled = enable;
        }


        /// <summary>
        /// Gets the metadata for the speech recognizer (acoustic model) most suitable to
        /// process audio from Kinect device.
        /// </summary>
        /// <returns>
        /// RecognizerInfo if found, <code>null</code> otherwise.
        /// </returns>
        private static RecognizerInfo GetKinectRecognizer()
        {
            foreach (RecognizerInfo recognizer in SpeechRecognitionEngine.InstalledRecognizers())
            {
                string value;
                recognizer.AdditionalInfo.TryGetValue("Kinect", out value);
                if ("True".Equals(value, StringComparison.OrdinalIgnoreCase) && "en-US".Equals(recognizer.Culture.Name, StringComparison.OrdinalIgnoreCase))
                {
                    return recognizer;
                }
            }

            return null;
        }


        /// <summary>
        /// Handler for recognized speech events.
        /// </summary>
        /// <param name="sender">object sending the event.</param>
        /// <param name="e">event arguments.</param>
        private void SpeechRecognized(object sender, SpeechRecognizedEventArgs e)
        {
            if (!speechRecognitionEnabled)
            {
                return;
            }

            // Check if the speech synthesizer is already speaking. Ignore any detected 
            // speech during this time, since it might have been generated by the synthesizer.
            lock (speechLock)
            {
                if (synthesizerIsSpeaking)
                {
                    ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Warning, "SpeechRecognized: Ignored word '" + e.Result.Text + "', since synthesizer is speaking.");

                    return;
                }
            }

            ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Info, "SpeechRecognized: Detected word '" + e.Result.Text + "' with confidence " + e.Result.Confidence.ToString());

            // Speech utterance confidence below which we treat speech as if it hadn't been heard
            const double ConfidenceThreshold = 0.5;

            if (e.Result.Confidence >= ConfidenceThreshold)
            {
                string sequenceName = "";

                // Perform the specified Sequence depending on the recognized speech.

                switch (e.Result.Semantics.Value.ToString())
                {
                    case "Hello":
                        sequenceName = "Hello";
                        break;

                        
                    case "HowAreUDoing":
                        sequenceName = "HowAreUDoing";
                        break;

                    case "Introduce":
                        sequenceName = "Introduce";
                        break;


                    case "Number":
                        sequenceName = "Number";
                        break;

                    case "DoWhat":
                        sequenceName = "DoWhat";
                        break;

                    case "Thanks":
                        sequenceName = "Thanks";
                        break;

                    case "Count":
                        sequenceName = "Count";
                        break;

                    case "OneAndTwo":
                        sequenceName = "OneAndTwo";
                        break;

                    case "FiveMinusOne":
                        sequenceName = "FiveMinusOne";
                        break;

                    case "Gesture":
                        sequenceName = "Gesture";
                        break;

                    case "NextOne":
                        sequenceName = "NextOne";
                        break;

                    case "NextTwo":
                        sequenceName = "NextTwo";
                        break;

                    case "NextThree":
                        sequenceName = "NextThree";
                        break;

                     case "PlayHarp":
                        sequenceName = "PlayHarp";
                        break;

                    case "PLAYMUSIC":
                        sequenceName = "PLAYMUSIC";
                        break;

                    case "lab":
                        sequenceName = "lab";
                        break;

                    case "GoAhead":
                        sequenceName = "GoAhead";
                        break;
                        
                    case "NAME":
                        sequenceName = "NAME";
                        break;

                    case "Bye":
                        sequenceName = "Bye";
                        break;

                    case "GoodMorning":
                        sequenceName = "GoodMorning";
                        break;

                    case "URwelcome":
                        sequenceName = "URwelcome";
                        break;

                    case "GladSU":
                        sequenceName = "GladSU";
                        break;

                    case "DEFAULT":
                        sequenceName = "DEFAULT";
                        break;
                }

                if (sequenceName != "")
                {
                    sequenceProcessor.RunSequence(sequenceName);
                }
            }
            else
            {
                // If speech is not recognized, then use a random default response.

                Random randomGenerator = new Random();
                int randomSequenceNumber = randomGenerator.Next(1, 5);
                string sequenceName = "DEFAULT" + randomSequenceNumber.ToString();

                if (sequenceName != "")
                {
                    sequenceProcessor.RunSequence(sequenceName);
                }
            }
        }

        /// <summary>
        /// Handler for rejected speech events.
        /// </summary>
        /// <param name="sender">object sending the event.</param>
        /// <param name="e">event arguments.</param>
        private void SpeechRejected(object sender, SpeechRecognitionRejectedEventArgs e)
        {
        }


        private void SpeakStarted(object sender, SpeakStartedEventArgs e)
        {
            //ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Info, "SpeakStarted");

            lock (speechLock)
            {
                synthesizerIsSpeaking = true;
            }
        }
        
        private void SpeakCompleted(object sender, SpeakCompletedEventArgs e)
        {
            //ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Info, "SpeakCompleted");

            lock (speechLock)
            {
                synthesizerIsSpeaking = false;
            }
        }

        public event EventHandler<SkeletonFrameReadyEventArgs> SkeletonFrameReady
        {
            add { sensor.SkeletonFrameReady += value; }
            remove { sensor.SkeletonFrameReady -= value; }
        }

        public DepthImagePoint MapSkeletonPointToDepthPoint(SkeletonPoint skelpoint, DepthImageFormat depthImageFormat)
        {
            return sensor.CoordinateMapper.MapSkeletonPointToDepthPoint(skelpoint, depthImageFormat);
        }

        // This is used to track whether a specific gesture has been triggered, 
        // and to prevent repeated triggering until the conditions are no longer met.
        private class GestureTrigger
        {
            bool hasBeenTriggered = false;

            public void Evaluate(SequenceProcessor sequenceProcessor, bool triggerCondition, string sequenceName)
            {
                if (triggerCondition)
                {
                    if (!hasBeenTriggered)
                    {
                        hasBeenTriggered = true;

                        sequenceProcessor.RunSequence(sequenceName);
                    }
                }
                else
                {
                    hasBeenTriggered = false;
                }
            }
        }

        int trackedSkeletonId = 0;

        // Gesture trigger definitions.
        //GestureTrigger leftHandGesture = new GestureTrigger();
        //GestureTrigger rightHandGesture = new GestureTrigger();
        GestureTrigger RightHandAboveHead = new GestureTrigger();
        GestureTrigger RightHandBetweenSpineAndShoulderCenter = new GestureTrigger();
        GestureTrigger LeftElbowAboveLeftShoulder = new GestureTrigger();
        GestureTrigger WalkCloseToKinect = new GestureTrigger();
        // [Add new GestureTriggers here]

        // Raised whenever new skeleton data arrives from the Kinect sensor.
        // Updates person tracking and performs gesture recognition.
        private void SensorSkeletonFrameReady(object sender, SkeletonFrameReadyEventArgs e)
        {
            Skeleton[] skeletons = new Skeleton[0];

            using (SkeletonFrame skeletonFrame = e.OpenSkeletonFrame())
            {
                if (skeletonFrame != null)
                {
                    skeletons = new Skeleton[skeletonFrame.SkeletonArrayLength];
                    skeletonFrame.CopySkeletonDataTo(skeletons);
                }
            }

            if (skeletons.Length != 0)
            {
                bool isTrackingSameSkeleton = false;
                int skeletonIdClosestToCenter = 0;
                float shortestDistanceFromCenter = 0;

                // First, check if this skeleton frame contains the skeleton that we were already tracking.
                foreach (Skeleton skel in skeletons)
                {
                    if (skel.TrackingState == SkeletonTrackingState.Tracked)
                    {
                        if (skel.TrackingId == trackedSkeletonId)
                        {
                            isTrackingSameSkeleton = true;
                            break;
                        }
                        else if (skeletonIdClosestToCenter == 0 || Math.Abs(skel.Joints[JointType.ShoulderCenter].Position.X) < shortestDistanceFromCenter)
                        {
                            skeletonIdClosestToCenter = skel.TrackingId;
                            shortestDistanceFromCenter = Math.Abs(skel.Joints[JointType.ShoulderCenter].Position.X);
                        }
                    }
                }

                // If the skeleton frame does not contain the one we were tracking, then track the skeleton that is closest to the center of the image (x-axis).
                if (!isTrackingSameSkeleton)
                {
                    trackedSkeletonId = skeletonIdClosestToCenter;

                    ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Debug, "SensorSkeletonFrameReady() now tracking skeleton ID " + trackedSkeletonId.ToString() + ".");
                }

                // Update person tracking and check gestures from the tracked skeleton.
                foreach (Skeleton skel in skeletons)
                {
                    // Only pay attention to the tracked skeleton.
                    if (skel.TrackingId == trackedSkeletonId)
                    {
                        // Only pay attention to the skeleton if it is within the right and left edges of the view.
                        // This should help prevent detecting false gestures, due to the skeleton being scrambled as it leaves the viewing area.
                        if ((skel.ClippedEdges & FrameEdges.Left) == 0 && (skel.ClippedEdges & FrameEdges.Right) == 0)
                        {
                            personTracking.UpdatePosition(skel.Joints[JointType.ShoulderCenter].Position);

                            if (gestureRecognitionEnabled)
                            {
                                // Check gesture recognition conditions.

                                //leftHandGesture.Evaluate(sequenceProcessor, skel.Joints[JointType.HandLeft].Position.Y > skel.Joints[JointType.ElbowLeft].Position.Y, "Left Hand Raised");

                                //rightHandGesture.Evaluate(sequenceProcessor, skel.Joints[JointType.HandRight].Position.Y > skel.Joints[JointType.ElbowRight].Position.Y, "Right Hand Raised");

                                RightHandAboveHead.Evaluate(sequenceProcessor, skel.Joints[JointType.HandRight].Position.Y > skel.Joints[JointType.Head].Position.Y, "Hello");

                                RightHandBetweenSpineAndShoulderCenter.Evaluate(sequenceProcessor, skel.Joints[JointType.HandRight].Position.Y > skel.Joints[JointType.Spine].Position.Y && skel.Joints[JointType.HandRight].Position.Y < skel.Joints[JointType.ShoulderCenter].Position.Y && skel.Joints[JointType.HandRight].Position.X < (skel.Joints[JointType.Spine].Position.X + 0.1), "PLAYMUSIC");

                                LeftElbowAboveLeftShoulder.Evaluate(sequenceProcessor, skel.Joints[JointType.ElbowLeft].Position.Y > skel.Joints[JointType.ShoulderLeft].Position.Y, "NeckRotate");

                                WalkCloseToKinect.Evaluate(sequenceProcessor, skel.Joints[JointType.Spine].Position.Z < 1.5, "ArmRaise");

                                // [Add new Evaluate calls here]
                            }
                        }
                        else
                        {
                            ErrorLogging.AddMessage(ErrorLogging.LoggingLevel.Debug, "SensorSkeletonFrameReady() skipped update due to skeleton being outside of view.");
                        }

                        break;
                    }
                }
            }
        }
    }
}
